{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOoNrk9Foxh2",
        "outputId": "2e2b4366-5872-46df-e711-a37a18898236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics==8.0.106\n",
            "  Downloading ultralytics-8.0.106-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (4.66.6)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (0.13.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.106) (2.19.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.106) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.106) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.106) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.106) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.106) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.106) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.106) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.106) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.0.106) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.0.106) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.106) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.106) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.106) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.106) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.106) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.106) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.106) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.106) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.106) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.106) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->ultralytics==8.0.106) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics==8.0.106) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->ultralytics==8.0.106) (3.0.2)\n",
            "Downloading ultralytics-8.0.106-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ultralytics\n",
            "Successfully installed ultralytics-8.0.106\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics==8.0.106"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics"
      ],
      "metadata": {
        "id": "YB-nJV-ao7Ia"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics.vit import SAM\n",
        "\n",
        "model = SAM('sam_b.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifX1ycw4o_ga",
        "outputId": "abadc8d1-3937-42f9-bdcc-89da9ab201dc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ultralytics/vit/sam/build.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict('/content/images/dog2.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc3vd4QnpDpE",
        "outputId": "b18cc19a-b402-4356-a8c0-595f8d67274a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.106 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "image 1/1 /content/images/dog2.jpg: 200x3 6345.3ms\n",
            "Speed: 0.0ms preprocess, 6345.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/segment/predict\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ultralytics.yolo.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: None\n",
              " keypoints: None\n",
              " keys: ['masks']\n",
              " masks: ultralytics.yolo.engine.results.Masks object\n",
              " names: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23}\n",
              " orig_img: array([[[166, 183, 179],\n",
              "         [124, 139, 135],\n",
              "         [126, 140, 136],\n",
              "         ...,\n",
              "         [255, 254, 255],\n",
              "         [255, 254, 255],\n",
              "         [255, 254, 255]],\n",
              " \n",
              "        [[180, 195, 191],\n",
              "         [136, 150, 146],\n",
              "         [152, 163, 160],\n",
              "         ...,\n",
              "         [255, 254, 255],\n",
              "         [255, 254, 255],\n",
              "         [255, 254, 255]],\n",
              " \n",
              "        [[154, 166, 160],\n",
              "         [133, 143, 137],\n",
              "         [224, 230, 225],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[222, 207, 204],\n",
              "         [222, 207, 204],\n",
              "         [221, 206, 203],\n",
              "         ...,\n",
              "         [227, 215, 211],\n",
              "         [224, 212, 208],\n",
              "         [198, 186, 182]],\n",
              " \n",
              "        [[219, 204, 201],\n",
              "         [218, 203, 200],\n",
              "         [218, 203, 200],\n",
              "         ...,\n",
              "         [225, 209, 203],\n",
              "         [219, 203, 197],\n",
              "         [224, 208, 202]],\n",
              " \n",
              "        [[220, 205, 202],\n",
              "         [220, 205, 202],\n",
              "         [221, 206, 203],\n",
              "         ...,\n",
              "         [231, 213, 206],\n",
              "         [231, 213, 206],\n",
              "         [223, 205, 198]]], dtype=uint8)\n",
              " orig_shape: (133, 200)\n",
              " path: '/content/images/dog2.jpg'\n",
              " probs: None\n",
              " speed: {'preprocess': 0.01811981201171875, 'inference': 6345.272064208984, 'postprocess': 0.8032321929931641}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics.yolo.data.annotator import auto_annotate"
      ],
      "metadata": {
        "id": "kgPO17oIpfxU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_annotate(data=\"images\", det_model=\"yolov8x.pt\", sam_model='sam_b.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR4d0a6Rp0YL",
        "outputId": "70b7a743-0f8f-400e-a735-9f6c011b8fc3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.106 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "image 1/4 /content/images/car.jpg: 384x640 1 truck, 57.7ms\n",
            "image 2/4 /content/images/cat.jpg: 480x640 1 cat, 43.0ms\n",
            "image 3/4 /content/images/dog1.jpg: 640x512 1 dog, 45.6ms\n",
            "image 4/4 /content/images/dog2.jpg: 448x640 1 dog, 43.1ms\n",
            "Speed: 2.7ms preprocess, 47.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np  # Add this for handling polygons\n",
        "\n",
        "def draw_yolo_labels(image_path, label_path, output_path, class_names=None):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Read the label file\n",
        "    with open(label_path, \"r\") as file:\n",
        "        labels = file.readlines()\n",
        "\n",
        "    for label in labels:\n",
        "        label_parts = label.strip().split()\n",
        "        class_id = int(label_parts[0])  # Extract the class ID\n",
        "\n",
        "        # Check if it is a polygon (more than 4 values after class_id)\n",
        "        if len(label_parts) > 5:\n",
        "            # Extract polygon points\n",
        "            points = list(map(float, label_parts[1:]))\n",
        "            polygon = []\n",
        "            for i in range(0, len(points), 2):\n",
        "                x = int(points[i] * width)\n",
        "                y = int(points[i + 1] * height)\n",
        "                polygon.append((x, y))\n",
        "\n",
        "            # Draw the polygon\n",
        "            polygon = np.array(polygon, np.int32)\n",
        "            polygon = polygon.reshape((-1, 1, 2))\n",
        "            cv2.polylines(image, [polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        else:\n",
        "            # Extract bounding box coordinates\n",
        "            x_center, y_center, box_width, box_height = map(float, label_parts[1:])\n",
        "            x1 = int((x_center - box_width / 2) * width)\n",
        "            y1 = int((y_center - box_height / 2) * height)\n",
        "            x2 = int((x_center + box_width / 2) * width)\n",
        "            y2 = int((y_center + box_height / 2) * height)\n",
        "\n",
        "            # Draw the bounding box\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        # Add class name if provided\n",
        "        if class_names:\n",
        "            class_name = class_names[class_id] if class_id < len(class_names) else str(class_id)\n",
        "            if len(label_parts) > 5:  # Polygon case\n",
        "                text_x, text_y = polygon[0][0][0], polygon[0][0][1]\n",
        "            else:  # Bounding box case\n",
        "                text_x, text_y = x1, y1\n",
        "            cv2.putText(image, class_name, (text_x, text_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Save the image with annotations\n",
        "    cv2.imwrite(output_path, image)\n",
        "\n",
        "\n",
        "# List of images and labels\n",
        "images = [\"/content/images/dog1.jpg\", \"/content/images/dog2.jpg\"]\n",
        "labels = [\"/content/labels/dog1.txt\", \"/content/labels/dog2.txt\"]\n",
        "output_dir = \"/content/annotated_images/\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each image and label\n",
        "for image_path, label_path in zip(images, labels):\n",
        "    output_path = os.path.join(output_dir, os.path.basename(image_path))  # Save with the same name as input image\n",
        "    draw_yolo_labels(image_path, label_path, output_path, class_names='dog')  # Add class_names if available\n"
      ],
      "metadata": {
        "id": "v9caX0UEp4Bu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np  # For handling polygons\n",
        "from ultralytics import YOLO  # YOLO library for extracting class names\n",
        "\n",
        "def draw_yolo_labels(image_path, label_path, output_path, class_names=None):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Read the label file\n",
        "    with open(label_path, \"r\") as file:\n",
        "        labels = file.readlines()\n",
        "\n",
        "    for label in labels:\n",
        "        label_parts = label.strip().split()\n",
        "        class_id = int(label_parts[0])  # Extract the class ID\n",
        "\n",
        "        # Check if it is a polygon (more than 4 values after class_id)\n",
        "        if len(label_parts) > 5:\n",
        "            # Extract polygon points\n",
        "            points = list(map(float, label_parts[1:]))\n",
        "            polygon = []\n",
        "            for i in range(0, len(points), 2):\n",
        "                x = int(points[i] * width)\n",
        "                y = int(points[i + 1] * height)\n",
        "                polygon.append((x, y))\n",
        "\n",
        "            # Draw the polygon\n",
        "            polygon = np.array(polygon, np.int32)\n",
        "            polygon = polygon.reshape((-1, 1, 2))\n",
        "            cv2.polylines(image, [polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        else:\n",
        "            # Extract bounding box coordinates\n",
        "            x_center, y_center, box_width, box_height = map(float, label_parts[1:])\n",
        "            x1 = int((x_center - box_width / 2) * width)\n",
        "            y1 = int((y_center - box_height / 2) * height)\n",
        "            x2 = int((x_center + box_width / 2) * width)\n",
        "            y2 = int((y_center + box_height / 2) * height)\n",
        "\n",
        "            # Draw the bounding box\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        # Add class name if provided\n",
        "        if class_names:\n",
        "            class_name = class_names[class_id] if class_id < len(class_names) else str(class_id)\n",
        "            if len(label_parts) > 5:  # Polygon case\n",
        "                text_x, text_y = polygon[0][0][0], polygon[0][0][1]\n",
        "            else:  # Bounding box case\n",
        "                text_x, text_y = x1, y1\n",
        "            # Change text color to blue (BGR: 255, 0, 0)\n",
        "            cv2.putText(image, class_name, (text_x, text_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "    # Save the image with annotations\n",
        "    cv2.imwrite(output_path, image)\n",
        "\n",
        "\n",
        "# Load the YOLO model\n",
        "yolo_model = YOLO(\"yolov8n.pt\")  # Load YOLOv8 model (use your desired model here)\n",
        "class_names = yolo_model.names  # Extract class names from the YOLO model\n",
        "\n",
        "# List of images and labels\n",
        "images = [\"/content/images/dog1.jpg\", \"/content/images/dog2.jpg\",\"/content/images/cat.jpg\"]\n",
        "labels = [\"/content/labels/dog1.txt\", \"/content/labels/dog2.txt\",\"/content/labels/cat.txt\"]\n",
        "output_dir = \"/content/annotated_images/\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each image and label\n",
        "for image_path, label_path in zip(images, labels):\n",
        "    output_path = os.path.join(output_dir, os.path.basename(image_path))  # Save with the same name as input image\n",
        "    draw_yolo_labels(image_path, label_path, output_path, class_names=class_names)\n"
      ],
      "metadata": {
        "id": "-I-S5GqjtCO4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(class_names)  # Check if \"cat\" exists in the loaded YOLO model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFJs0sF4hl3D",
        "outputId": "2fe5aab0-b146-43a1-8816-d3f3a4a1cbac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\")  # Use the appropriate model file (e.g., yolov8s.pt, yolov8m.pt)\n",
        "\n",
        "# Path to the image you want to test\n",
        "image_path = \"/content/images/cat.jpg\"\n",
        "\n",
        "# Run prediction\n",
        "results = yolo_model.predict(source=image_path, conf=0.25, save=True, save_txt=True)  # Adjust confidence as needed\n",
        "\n",
        "# Print the results\n",
        "print(\"Prediction Results:\")\n",
        "for result in results:\n",
        "    print(result.boxes)  # Prints the bounding boxes and class IDs\n",
        "\n",
        "# Extract detected classes\n",
        "print(\"\\nDetected Classes:\")\n",
        "for result in results:\n",
        "    for box in result.boxes.data.tolist():\n",
        "        class_id = int(box[5])  # Assuming class ID is in index 5\n",
        "        confidence = box[4]  # Assuming confidence is in index 4\n",
        "        print(f\"Class ID: {class_id}, Confidence: {confidence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh-uFogRiy3Q",
        "outputId": "a4a57e9c-1422-44e5-c949-891aee183f7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/images/cat.jpg: 480x640 1 cat, 61.2ms\n",
            "Speed: 2.9ms preprocess, 61.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            "1 label saved to runs/detect/predict/labels\n",
            "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction Results:\n",
            "ultralytics.yolo.engine.results.Boxes object with attributes:\n",
            "\n",
            "boxes: tensor([[ 60.1292,   9.0095, 380.0529, 375.0000,   0.8288,  15.0000]], device='cuda:0')\n",
            "cls: tensor([15.], device='cuda:0')\n",
            "conf: tensor([0.8288], device='cuda:0')\n",
            "data: tensor([[ 60.1292,   9.0095, 380.0529, 375.0000,   0.8288,  15.0000]], device='cuda:0')\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: tensor([375, 500], device='cuda:0')\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[220.0910, 192.0047, 319.9238, 365.9905]], device='cuda:0')\n",
            "xywhn: tensor([[0.4402, 0.5120, 0.6398, 0.9760]], device='cuda:0')\n",
            "xyxy: tensor([[ 60.1292,   9.0095, 380.0529, 375.0000]], device='cuda:0')\n",
            "xyxyn: tensor([[0.1203, 0.0240, 0.7601, 1.0000]], device='cuda:0')\n",
            "\n",
            "Detected Classes:\n",
            "Class ID: 15, Confidence: 0.8288013935089111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def draw_yolo_labels(image_path, label_path, output_path, model_path=\"yolov8n.pt\"):\n",
        "    \"\"\"\n",
        "    Annotates an image using the provided YOLO labels.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        label_path (str): Path to the YOLO label file.\n",
        "        output_path (str): Path to save the annotated image.\n",
        "        model_path (str): Path to the YOLO model (unused in this version).\n",
        "    \"\"\"\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Get class names from the YOLO model (this part is optional if you just need the labels)\n",
        "    yolo_model = YOLO(model_path)\n",
        "    class_names = yolo_model.names\n",
        "\n",
        "    # 1. Draw from label file if it exists\n",
        "    if os.path.exists(label_path):\n",
        "        with open(label_path, \"r\") as file:\n",
        "            labels = file.readlines()\n",
        "        for label in labels:\n",
        "            label_parts = label.strip().split()\n",
        "            class_id = int(label_parts[0])  # Class ID\n",
        "\n",
        "            if len(label_parts) == 5:  # Bounding box format\n",
        "                x_center, y_center, box_width, box_height = map(float, label_parts[1:])\n",
        "\n",
        "                # Convert YOLO format to pixel coordinates\n",
        "                x1 = int((x_center - box_width / 2) * width)\n",
        "                y1 = int((y_center - box_height / 2) * height)\n",
        "                x2 = int((x_center + box_width / 2) * width)\n",
        "                y2 = int((y_center + box_height / 2) * height)\n",
        "\n",
        "                # Draw bounding box\n",
        "                cv2.rectangle(image, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)  # Blue for labels\n",
        "\n",
        "                # Add class name\n",
        "                class_name = class_names[class_id] if class_id < len(class_names) else str(class_id)\n",
        "                cv2.putText(image, f\"{class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "            elif len(label_parts) > 5:  # Polygon format\n",
        "                points = list(map(float, label_parts[1:]))\n",
        "                polygon = []\n",
        "                for i in range(0, len(points), 2):\n",
        "                    x = int(points[i] * width)\n",
        "                    y = int(points[i + 1] * height)\n",
        "                    polygon.append((x, y))\n",
        "\n",
        "                # Draw the polygon\n",
        "                polygon = np.array(polygon, np.int32).reshape((-1, 1, 2))\n",
        "                cv2.polylines(image, [polygon], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "\n",
        "                # Add class name\n",
        "                class_name = class_names[class_id] if class_id < len(class_names) else str(class_id)\n",
        "                text_x, text_y = polygon[0][0]\n",
        "                cv2.putText(image, f\"{class_name}\", (text_x, text_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "    # Save the annotated image\n",
        "    cv2.imwrite(output_path, image)\n",
        "    print(f\"Annotated image saved at: {output_path}\")\n",
        "\n",
        "\n",
        "# Directories containing images and labels\n",
        "images_dir = \"/content/images\"\n",
        "labels_dir = \"/content/labels\"\n",
        "output_dir = \"/content/outputs\"  # Path to save the annotated images\n",
        "model_path = \"yolov8n.pt\"  # Path to your YOLOv8 model (unused in this version)\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each image and its corresponding label file\n",
        "for image_filename in os.listdir(images_dir):\n",
        "    # Match the image with its label (assumes the label file has the same name as the image file)\n",
        "    image_path = os.path.join(images_dir, image_filename)\n",
        "    label_filename = os.path.splitext(image_filename)[0] + \".txt\"\n",
        "    label_path = os.path.join(labels_dir, label_filename)\n",
        "\n",
        "    # Ensure label file exists\n",
        "    if os.path.exists(label_path):\n",
        "        output_filename = os.path.splitext(image_filename)[0] + \"_annotated.jpg\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        # Run the annotation function\n",
        "        draw_yolo_labels(image_path, label_path, output_path, model_path=model_path)\n",
        "    else:\n",
        "        print(f\"Label file not found for {image_filename}. Skipping.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oo4wukzjVUY",
        "outputId": "f0d2c1a5-fdd6-4005-a284-511df1bcc597"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotated image saved at: /content/outputs/car_annotated.jpg\n",
            "Annotated image saved at: /content/outputs/cat_annotated.jpg\n",
            "Annotated image saved at: /content/outputs/dog2_annotated.jpg\n",
            "Annotated image saved at: /content/outputs/dog1_annotated.jpg\n"
          ]
        }
      ]
    }
  ]
}